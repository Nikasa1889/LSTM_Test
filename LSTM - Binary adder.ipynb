{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "#add binary string a, and b to create c.\n",
    "def add(a, b):\n",
    "    l = max(len(a), len(b));\n",
    "    a = int(a, 2);\n",
    "    b = int(b, 2);\n",
    "    c = bin(a+b)[2:].zfill(l);\n",
    "    return c;\n",
    "\n",
    "def str2np(a):\n",
    "    result = [int(x) for x in a]\n",
    "    return result;\n",
    "\n",
    "def lstofstr2tensor(lstofstr):\n",
    "    tensor = [str2np(string) for string in lstofstr]\n",
    "    return tensor\n",
    "\n",
    "def merge_str2list(str1, str2):\n",
    "    result = [[int(str1[i]), int(str2[i])] for i in xrange(len(str1))]\n",
    "    return result\n",
    "N_bit = 15;\n",
    "#To prevent overloading bit on result, we only generate N_bit-1 string\n",
    "formatting_string = ('{0:0'+str(N_bit)+'b}')\n",
    "input_a = [formatting_string.format(i) for i in xrange(2**(N_bit-1))];\n",
    "input_b = [formatting_string.format(i) for i in xrange(2**(N_bit-1))];\n",
    "shuffle(input_a);\n",
    "shuffle(input_b);\n",
    "\n",
    "N_TOTAL = 2000;\n",
    "\n",
    "input_a = input_a[:N_TOTAL];\n",
    "input_b = input_b[:N_TOTAL];\n",
    "target_c = [add(input_a[i], input_b[i]) for i in xrange(N_TOTAL)];\n",
    "\n",
    "#Need to reverse the binary string, since we do the calculation from right to left\n",
    "input_a = [a[::-1] for a in input_a];\n",
    "input_b = [b[::-1] for b in input_b];\n",
    "target_c = [c[::-1] for c in target_c];\n",
    "\n",
    "#Merge string a and b to one input\n",
    "input_ab = [merge_str2list(input_a[i], input_b[i]) for i in xrange(N_TOTAL)];\n",
    "\n",
    "target_c = lstofstr2tensor(target_c);\n",
    "\n",
    "#Convert to numpy array\n",
    "input_ab = np.asarray(input_ab)\n",
    "target_c = np.asarray(target_c)\n",
    "\n",
    "#Split to training and testing dataset\n",
    "N_TRAIN = 1500;\n",
    "test_input_ab = input_ab[N_TRAIN:]\n",
    "test_target_c = target_c[N_TRAIN:] \n",
    "\n",
    "train_input_ab = input_ab[:N_TRAIN]\n",
    "train_target_c = target_c[:N_TRAIN] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tensorflow for LSTM\n",
    "#### First we define the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Placeholder for input and target, [Batch Size, Sequence Length, Input Dimension]\n",
    "data = tf.placeholder(tf.float32, [None, N_bit, 2])\n",
    "target = tf.placeholder(tf.float32, [None, N_bit])\n",
    "\n",
    "num_hidden = 5\n",
    "time_steps = N_bit\n",
    "\n",
    "lstmCell = tf.nn.rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "sequence_length \n",
    "#unroll the LSTM network\n",
    "outputs, states = tf.nn.dynamic_rnn(cell=lstmCell, inputs=data, dtype=tf.float32)\n",
    "\n",
    "# Define weights\n",
    "out_size = 1;\n",
    "\n",
    "# Weights for each timestep output, using the same weight\n",
    "weight = tf.Variable(tf.truncated_normal([num_hidden, out_size], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "\n",
    "outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "prediction = tf.sigmoid(tf.matmul(outputs, weight) + bias)\n",
    "prediction = tf.reshape(prediction, [-1, time_steps, out_size])\n",
    "prediction = tf.squeeze(prediction)\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(target * tf.log(prediction) + (1-target)*tf.log(1-prediction))\n",
    "#cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#cross_entropy = -tf.reduce_sum(target * tf.log(prediction))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "final_prediction = prediction>0.5;\n",
    "mistakes = tf.not_equal(target>0.5, final_prediction) #careful when target is still float32\n",
    "error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we execute the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  0\n",
      "Epoch -  1\n",
      "Epoch -  2\n",
      "Epoch -  3\n",
      "Epoch -  4\n",
      "Epoch -  5\n",
      "Epoch -  6\n",
      "Epoch -  7\n",
      "Epoch -  8\n",
      "Epoch -  9\n",
      "Epoch -  10\n",
      "Epoch -  11\n",
      "Epoch -  12\n",
      "Epoch -  13\n",
      "Epoch -  14\n",
      "Epoch -  15\n",
      "Epoch -  16\n",
      "Epoch -  17\n",
      "Epoch -  18\n",
      "Epoch -  19\n",
      "Epoch -  20\n",
      "Epoch -  21\n",
      "Epoch -  22\n",
      "Epoch -  23\n",
      "Epoch -  24\n",
      "Epoch -  25\n",
      "Epoch -  26\n",
      "Epoch -  27\n",
      "Epoch -  28\n",
      "Epoch -  29\n",
      "Epoch -  30\n",
      "Epoch -  31\n",
      "Epoch -  32\n",
      "Epoch -  33\n",
      "Epoch -  34\n",
      "Epoch -  35\n",
      "Epoch -  36\n",
      "Epoch -  37\n",
      "Epoch -  38\n",
      "Epoch -  39\n",
      "Epoch -  40\n",
      "Epoch -  41\n",
      "Epoch -  42\n",
      "Epoch -  43\n",
      "Epoch -  44\n",
      "Epoch -  45\n",
      "Epoch -  46\n",
      "Epoch -  47\n",
      "Epoch -  48\n",
      "Epoch -  49\n",
      "Epoch -  50\n",
      "Epoch -  51\n",
      "Epoch -  52\n",
      "Epoch -  53\n",
      "Epoch -  54\n",
      "Epoch -  55\n",
      "Epoch -  56\n",
      "Epoch -  57\n",
      "Epoch -  58\n",
      "Epoch -  59\n",
      "Epoch -  60\n",
      "Epoch -  61\n",
      "Epoch -  62\n",
      "Epoch -  63\n",
      "Epoch -  64\n",
      "Epoch -  65\n",
      "Epoch -  66\n",
      "Epoch -  67\n",
      "Epoch -  68\n",
      "Epoch -  69\n",
      "Epoch -  70\n",
      "Epoch -  71\n",
      "Epoch -  72\n",
      "Epoch -  73\n",
      "Epoch -  74\n",
      "Epoch -  75\n",
      "Epoch -  76\n",
      "Epoch -  77\n",
      "Epoch -  78\n",
      "Epoch -  79\n",
      "Epoch -  80\n",
      "Epoch -  81\n",
      "Epoch -  82\n",
      "Epoch -  83\n",
      "Epoch -  84\n",
      "Epoch -  85\n",
      "Epoch -  86\n",
      "Epoch -  87\n",
      "Epoch -  88\n",
      "Epoch -  89\n",
      "Epoch -  90\n",
      "Epoch -  91\n",
      "Epoch -  92\n",
      "Epoch -  93\n",
      "Epoch -  94\n",
      "Epoch -  95\n",
      "Epoch -  96\n",
      "Epoch -  97\n",
      "Epoch -  98\n",
      "Epoch -  99\n",
      "Epoch -  100\n",
      "Epoch -  101\n",
      "Epoch -  102\n",
      "Epoch -  103\n",
      "Epoch -  104\n",
      "Epoch -  105\n",
      "Epoch -  106\n",
      "Epoch -  107\n",
      "Epoch -  108\n",
      "Epoch -  109\n",
      "Epoch -  110\n",
      "Epoch -  111\n",
      "Epoch -  112\n",
      "Epoch -  113\n",
      "Epoch -  114\n",
      "Epoch -  115\n",
      "Epoch -  116\n",
      "Epoch -  117\n",
      "Epoch -  118\n",
      "Epoch -  119\n",
      "Epoch -  120\n",
      "Epoch -  121\n",
      "Epoch -  122\n",
      "Epoch -  123\n",
      "Epoch -  124\n",
      "Epoch -  125\n",
      "Epoch -  126\n",
      "Epoch -  127\n",
      "Epoch -  128\n",
      "Epoch -  129\n",
      "Epoch -  130\n",
      "Epoch -  131\n",
      "Epoch -  132\n",
      "Epoch -  133\n",
      "Epoch -  134\n",
      "Epoch -  135\n",
      "Epoch -  136\n",
      "Epoch -  137\n",
      "Epoch -  138\n",
      "Epoch -  139\n",
      "Epoch -  140\n",
      "Epoch -  141\n",
      "Epoch -  142\n",
      "Epoch -  143\n",
      "Epoch -  144\n",
      "Epoch -  145\n",
      "Epoch -  146\n",
      "Epoch -  147\n",
      "Epoch -  148\n",
      "Epoch -  149\n",
      "Epoch -  150\n",
      "Epoch -  151\n",
      "Epoch -  152\n",
      "Epoch -  153\n",
      "Epoch -  154\n",
      "Epoch -  155\n",
      "Epoch -  156\n",
      "Epoch -  157\n",
      "Epoch -  158\n",
      "Epoch -  159\n",
      "Epoch -  160\n",
      "Epoch -  161\n",
      "Epoch -  162\n",
      "Epoch -  163\n",
      "Epoch -  164\n",
      "Epoch -  165\n",
      "Epoch -  166\n",
      "Epoch -  167\n",
      "Epoch -  168\n",
      "Epoch -  169\n",
      "Epoch -  170\n",
      "Epoch -  171\n",
      "Epoch -  172\n",
      "Epoch -  173\n",
      "Epoch -  174\n",
      "Epoch -  175\n",
      "Epoch -  176\n",
      "Epoch -  177\n",
      "Epoch -  178\n",
      "Epoch -  179\n",
      "Epoch -  180\n",
      "Epoch -  181\n",
      "Epoch -  182\n",
      "Epoch -  183\n",
      "Epoch -  184\n",
      "Epoch -  185\n",
      "Epoch -  186\n",
      "Epoch -  187\n",
      "Epoch -  188\n",
      "Epoch -  189\n",
      "Epoch -  190\n",
      "Epoch -  191\n",
      "Epoch -  192\n",
      "Epoch -  193\n",
      "Epoch -  194\n",
      "Epoch -  195\n",
      "Epoch -  196\n",
      "Epoch -  197\n",
      "Epoch -  198\n",
      "Epoch -  199\n",
      "Epoch 200 error 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, ..., 0, 1, 0],\n",
       "       [1, 1, 1, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [1, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 1, ..., 0, 1, 1],\n",
       "       [0, 1, 1, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "no_of_batches = int(len(train_input_ab)/BATCH_SIZE)\n",
    "epoch = 200\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp, out = train_input_ab[ptr:ptr+BATCH_SIZE], train_target_c[ptr:ptr+BATCH_SIZE]\n",
    "        ptr+=BATCH_SIZE\n",
    "        sess.run(minimize,{data: inp, target: out})\n",
    "        #print(sess.run(cross_entropy,{data: inp, target: out}))\n",
    "    print \"Epoch - \",str(i)\n",
    "incorrect = sess.run(error,{data: test_input_ab, target: test_target_c})\n",
    "print('Epoch {:2d} error {:3.1f}%'.format(i + 1, 100 * incorrect))\n",
    "prediction = sess.run(final_prediction,{data: test_input_ab, target: test_target_c})\n",
    "prediction.astype(int)\n",
    "\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]]\n",
      "[0 1 1 0 0 0 1 0 0 0 1 0 1 0 1]\n",
      "[False  True  True False False False  True False False False  True False\n",
      "  True False  True]\n"
     ]
    }
   ],
   "source": [
    "#Some prediction examples:\n",
    "prediction = sess.run(final_prediction,{data: test_input_ab, target: test_target_c})\n",
    "print test_input_ab[59]\n",
    "print test_target_c[59]\n",
    "print prediction[59]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
